{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IaC: Infrastructure-as-code\n",
    "\n",
    "### This notebook handles the following tasks:<br>\n",
    "- Load all the DWH parameters from the configuration file <br>\n",
    "- Create clients for AWS services: S3, EC2, IAM and Redshift <br>\n",
    "- Create an IAM role to allow Redshift cluster to access S3 buckets <br>\n",
    "- Create a Redshift cluster <br>\n",
    "- Open an incoming TCP port to access the cluster ednpoint <br>\n",
    "- Connect to the cluster <br>\n",
    "- Delete the created cluster <br>\n",
    "- Delete all the created resources <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "import configparser\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DWH params from the configuration file (dwh.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilise a configparser object\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "# load configurated values into local variables\n",
    "\n",
    "KEY = config.get('AWS','KEY')\n",
    "SECRET = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE = config.get('DWH','DWH_CLUSTER_TYPE')\n",
    "DWH_NUMBER_NODES = config.get('DWH','DWH_NUMBER_NODES')\n",
    "DWH_NODE_TYPE = config.get('DWH','DWH_NODE_TYPE')\n",
    "DWH_IAM_ROLE_NAME = config.get('DWH','DWH_IAM_ROLE_NAME')\n",
    "DWH_CLUSTER_IDENTIFIER = config.get('DWH','DWH_CLUSTER_IDENTIFIER')\n",
    "\n",
    "DB_NAME = config.get('CLUSTER','DB_NAME')\n",
    "DB_USER = config.get('CLUSTER','DB_USER')\n",
    "DB_PASSWORD = config.get('CLUSTER','DB_PASSWORD')\n",
    "DB_PORT = config.get('CLUSTER','DB_PORT')\n",
    "\n",
    "LOG_DATA = config.get('S3','LOG_DATA')\n",
    "LOG_JSONPATH = config.get('S3','LOG_JSONPATH')\n",
    "SONG_DATA = config.get('S3','SONG_DATA')\n",
    "\n",
    "# view the values as a dataframe\n",
    "pd.DataFrame({'Param' : ['DWH_CLUSTER_TYPE', 'DWH_NUMBER_NODES', 'DWH_NODE_TYPE', 'DWH_CLUSTER_IDENTIFIER', 'DWH_IAM_ROLE_NAME', 'DB_NAME', 'DB_USER',\n",
    "                         'DB_PASSWORD', 'DB_PORT', 'LOG_DATA', 'LOG_JSONPATH', 'SONG_DATA'],\n",
    "             'Value' : [DWH_CLUSTER_TYPE, DWH_NUMBER_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_IAM_ROLE_NAME, DB_NAME, DB_USER, DB_PASSWORD,\n",
    "                       DB_PORT, LOG_DATA, LOG_JSONPATH, SONG_DATA]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create clients for S3, EC2, IAM and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilize clients for amazon services using boto3\n",
    "\n",
    "iam = boto3.client('iam',\n",
    "                    region_name = 'us-west-2',\n",
    "                    aws_access_key_id = KEY,\n",
    "                    aws_secret_access_key = SECRET)\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name = 'us-west-2',\n",
    "                       aws_access_key_id = KEY,\n",
    "                       aws_secret_access_key = SECRET)\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                 region_name = 'us-west-2',\n",
    "                 aws_access_key_id = KEY,\n",
    "                 aws_secret_access_key = SECRET)\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                    region_name = 'us-west-2',\n",
    "                    aws_access_key_id = KEY,\n",
    "                    aws_secret_access_key = SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Create an IAM role that makes Redshift able to access S3 bucket\n",
    "- Copy the RoleARN to dwh.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an IAM role\n",
    "\n",
    "try:\n",
    "    print('1.1 Creating an IAM role')\n",
    "    dwhRole = iam.create_role(\n",
    "                Path = '/',\n",
    "                RoleName = DWH_IAM_ROLE_NAME,\n",
    "                Description = 'Allow Redshift cluster to call AWS services on your behalf.',\n",
    "                AssumeRolePolicyDocument = json.dumps({'Statement': [{'Action': 'sts:AssumeRole',\n",
    "                                           'Effect': 'Allow',\n",
    "                                           'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "                                            'Version': '2012-10-17'}))\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "    \n",
    "# attach the role policy\n",
    "\n",
    "print('1.2 Attaching policy')\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "# get the IAM role ARN\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: Create Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Redshift cluster\n",
    "\n",
    "try:\n",
    "    response = redshift.create_cluster(\n",
    "                #HW\n",
    "                ClusterType=DWH_CLUSTER_TYPE,\n",
    "                NodeType=DWH_NODE_TYPE,\n",
    "                NumberOfNodes=int(DWH_NUMBER_NODES),\n",
    "\n",
    "                #Identifiers & Credentials\n",
    "                DBName=DB_NAME,\n",
    "                ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "                MasterUsername=DB_USER,\n",
    "                MasterUserPassword=DB_PASSWORD,\n",
    "        \n",
    "                #Roles (for s3 access)\n",
    "                IamRoles=[roleArn] ) \n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Check the cluster status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    \"\"\" \n",
    "    A function to print the cluster properties as a dataframe.\n",
    "    \n",
    "    Parameters: \n",
    "    props (properties): cluster properties which are the results of a redshift.describe.\n",
    "    \"\"\"\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Take a note of the cluster endpoint and role ARN\n",
    "- Run this cell when the cluster status becomes avialable <br>\n",
    "- Copy the DB_ENDPOINT to dwh.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the DATABASE ENDPOINT and IAM ROLE ARN values \n",
    "\n",
    "DB_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DB_ENDPOINT :: \", DB_ENDPOINT)\n",
    "print(\"ROLE_ARN :: \", ROLE_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: Open an incoming TCP port to access the cluster ednpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a TCP port to access the cluster endpoint\n",
    "\n",
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DB_PORT),\n",
    "        ToPort=int(DB_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4: Connect to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the cluster\n",
    "\n",
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DB_USER, DB_PASSWORD, DB_ENDPOINT, DB_PORT, DB_NAME)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5: Delete the created cluster and resources\n",
    "Only run when you finish with the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the created cluster\n",
    "# uncomment to run \n",
    "#redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print cluster properties to check it's statust\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the created resources\n",
    "# uncomment to run \n",
    "\n",
    "#iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "#iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
